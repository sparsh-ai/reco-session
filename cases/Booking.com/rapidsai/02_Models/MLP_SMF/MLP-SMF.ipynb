{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MIT License (MIT)\n",
    "\n",
    "Copyright (c) 2021 NVIDIA CORPORATION\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "fname = 'cpmp_092'\n",
    "\n",
    "checkpoint_path = Path('./checkpoints') / fname\n",
    "\n",
    "# if not checkpoint_path.exists():\n",
    "#     checkpoint_path.mkdir(parents=True)\n",
    "# else:\n",
    "#     sys.exit()\n",
    "    \n",
    "input_path = Path('../../00_Data/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOW_CITY_THR = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import cupy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_feature(df, groupby_col, col, offset, nan=-1, colname=''):\n",
    "    df[colname] = df[col].shift(offset)\n",
    "    df.loc[df[groupby_col]!=df[groupby_col].shift(offset), colname] = nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed_value):\n",
    "    random.seed(seed_value) # Python\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    torch.manual_seed(seed_value) # cpu  vars    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "    if torch.backends.cudnn.is_available:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return top4 metric\n",
    "# istest: flag to select if metric should be computed in 0:train, 1:test,\n",
    "# pos: select which city to calculate the metric, 0: last, 1: last-1, 2:last-2 , -1: all\n",
    "# the input `val` dataframe must contains the target `city_id` and the 4 recommendations as: rec0, res1, rec2 and rec3\n",
    "\n",
    "def top4_metric( val, istest=0, pos=0 , target='city_id'):\n",
    "    \n",
    "    if istest>=0:\n",
    "        val = val.loc[ (val.submission==0) & (val.istest == istest) ]\n",
    "    else:\n",
    "        val = val.loc[ (val.submission==0) ]\n",
    "\n",
    "    if pos >= 0:\n",
    "        top1 = val.loc[val.icount==pos,target] == val.loc[val.icount==pos,'rec0']\n",
    "        top2 = val.loc[val.icount==pos,target] == val.loc[val.icount==pos,'rec1']\n",
    "        top3 = val.loc[val.icount==pos,target] == val.loc[val.icount==pos,'rec2']\n",
    "        top4 = val.loc[val.icount==pos,target] == val.loc[val.icount==pos,'rec3']\n",
    "    else:\n",
    "        top1 = val[target] == val['rec0']\n",
    "        top2 = val[target] == val['rec1']\n",
    "        top3 = val[target] == val['rec2']\n",
    "        top4 = val[target] == val['rec3']\n",
    "        \n",
    "    return (top1|top2|top3|top4).mean()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = cudf.read_csv(input_path / 'train_and_test_2.csv')\n",
    "print(raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.loc[raw['city_id'] == 0, 'city_id'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw[(raw.istest == 0) | (raw.icount > 0)].groupby('city_id').utrip_id.count().reset_index()\n",
    "df\n",
    "\n",
    "df.columns = ['city_id', 'city_count']\n",
    "raw = raw.merge(df, how='left', on='city_id')\n",
    "raw.loc[raw.city_count <= LOW_CITY_THR, 'city_id'] = -1\n",
    "raw = raw.sort_values(['utrip_id', 'checkin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATS = ['city_id', 'hotel_country', 'booker_country', 'device_class']\n",
    "MAPS = []\n",
    "for c in CATS:\n",
    "    raw[c+'_'], mp = raw[c].factorize()\n",
    "    MAPS.append(mp)\n",
    "    print('created', c+'_')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOW_CITY = np.where(MAPS[0].to_pandas() == -1)[0][0]\n",
    "LOW_CITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CITIES = raw.city_id_.max()+1\n",
    "NUM_HOTELS = raw.hotel_country_.max()+1\n",
    "NUM_DEVICE = raw.device_class_.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['reverse'] = 0\n",
    "rev_raw = raw[raw.istest == 0].copy()\n",
    "rev_raw['reverse'] = 1\n",
    "rev_raw['utrip_id'] = rev_raw['utrip_id']+'_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = rev_raw['icount'].values.copy()\n",
    "rev_raw['icount'] = rev_raw['dcount']\n",
    "rev_raw['dcount'] = tmp\n",
    "rev_raw = rev_raw.sort_values(['utrip_id', 'dcount']).reset_index(drop=True)\n",
    "raw = cudf.concat([raw, rev_raw]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['sorting'] = cupy.asarray(range(raw.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['utrip_id'+'_'], mp = raw['utrip_id'].factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENGINEER LAG FEATURES\n",
    "LAGS=5\n",
    "lag_cities = []\n",
    "lag_countries = []\n",
    "\n",
    "for i in range(1,LAGS+1):\n",
    "    shift_feature(raw, 'utrip_id_', 'city_id_', i, NUM_CITIES, f'city_id_lag{i}')\n",
    "    lag_cities.append(f'city_id_lag{i}')\n",
    "    shift_feature(raw, 'utrip_id_', 'hotel_country_', i, NUM_HOTELS, f'country_lag{i}')\n",
    "    lag_countries.append(f'country_lag{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lag_countries = lag_countries[:1]\n",
    "lag_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpD = raw[raw['dcount']==0][['utrip_id', 'city_id_']]\n",
    "tmpD.columns = ['utrip_id', 'first_city']\n",
    "raw = raw.merge(tmpD,on='utrip_id',how='left')\n",
    "tmpD = raw[raw['dcount']==0][['utrip_id', 'hotel_country_']]\n",
    "tmpD.columns = ['utrip_id', 'first_country']\n",
    "raw = raw.merge(tmpD,on='utrip_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "raw['checkin'] = cudf.to_datetime(raw.checkin, format=\"%Y-%m-%d\")\n",
    "raw['checkout'] = cudf.to_datetime(raw.checkout, format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['mn'] = raw.checkin.dt.month\n",
    "raw['dy1'] = raw.checkin.dt.weekday\n",
    "raw['dy2'] = raw.checkout.dt.weekday\n",
    "raw['length'] = cupy.log1p((raw.checkout - raw.checkin).dt.days) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpD = raw[raw['dcount']==0][['utrip_id', 'checkin']]\n",
    "tmpD.columns = ['utrip_id', 'first_checkin']\n",
    "raw = raw.merge(tmpD,on='utrip_id',how='left')\n",
    "tmpD = raw[raw['icount']==0][['utrip_id', 'checkout']]\n",
    "tmpD.columns = ['utrip_id', 'last_checkout']\n",
    "raw = raw.merge(tmpD,on='utrip_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['trip_length'] = ((raw.last_checkout - raw.first_checkin).dt.days)\n",
    "raw['trip_length'] = cupy.log1p(cupy.abs(raw['trip_length'])) * cupy.sign(raw['trip_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpD = raw[raw['icount']==0][['utrip_id', 'checkin']]\n",
    "tmpD.columns = ['utrip_id', 'last_checkin']\n",
    "raw = raw.merge(tmpD,on='utrip_id',how='left')\n",
    "tmpD = raw[raw['dcount']==0][['utrip_id', 'checkout']]\n",
    "tmpD.columns = ['utrip_id', 'first_checkout']\n",
    "raw = raw.merge(tmpD,on='utrip_id',how='left')\n",
    "raw['trip_length'] = raw['trip_length'] - raw['trip_length'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw.sort_values('sorting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_feature(raw, 'utrip_id_', 'checkout', 1, None, f'checkout_lag{1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['lapse'] = (raw['checkin'] - raw['checkout_lag1'] ).dt.days.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENGINEER WEEKEND AND SEASON\n",
    "raw['day_name']= raw.checkin.dt.weekday\n",
    "raw['weekend']=raw['day_name'].isin([5,6]).astype('int8')\n",
    "df_season = cudf.DataFrame({'mn': range(1,13), 'season': ([0]*3)+([1]*3)+([2]*3)+([3]*3)})\n",
    "raw=raw.merge(df_season, how='left', on='mn')\n",
    "raw = raw.sort_values(['sorting'], ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(raw['lapse'].to_pandas(), bins=100, log=True)\n",
    "raw['lapse'].mean(), raw['lapse'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(raw['N'].to_pandas(), bins=100, log=True)\n",
    "raw['N'].mean(), raw['N'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['N'] = raw['N'] - raw['N'].mean()\n",
    "raw['N'] /= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(raw['trip_length'].to_pandas(), bins=100, log=True)\n",
    "raw['trip_length'].mean(), raw['length'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(raw['length'].to_pandas(), bins=100, log=True)\n",
    "raw['length'].mean(), raw['length'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['log_icount'] = cupy.log1p(raw['icount'])\n",
    "raw['log_dcount'] = cupy.log1p(raw['dcount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(raw['log_icount'].to_pandas(), bins=100, log=True)\n",
    "raw['log_icount'].mean(), raw['log_icount'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(raw['log_dcount'].to_pandas(), bins=100, log=True)\n",
    "raw['log_dcount'].mean(), raw['log_dcount'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['mn'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['dy1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['dy2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookingDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 target=None,\n",
    "                ):\n",
    "        super(BookingDataset, self).__init__()\n",
    "        self.lag_cities_ = data[lag_cities].values\n",
    "        self.mn = data['mn'].values - 1\n",
    "        self.dy1 = data['dy1'].values\n",
    "        self.dy2 = data['dy2'].values\n",
    "        self.length = data['length'].values\n",
    "        self.trip_length = data['trip_length'].values\n",
    "        self.N = data['N'].values\n",
    "        self.log_icount = data['log_icount'].values\n",
    "        self.log_dcount = data['log_dcount'].values\n",
    "        self.lag_countries_ = data[lag_countries].values\n",
    "        self.first_city = data['first_city'].values\n",
    "        self.first_country = data['first_country'].values\n",
    "        self.booker_country_ = data['booker_country_'].values\n",
    "        self.device_class_ = data['device_class_'].values\n",
    "        self.lapse = data['lapse'].values\n",
    "        self.season = data['season'].values\n",
    "        self.weekend = data['weekend'].values\n",
    "        if target is None:\n",
    "            self.target = None\n",
    "        else:\n",
    "            self.target = data[target].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.lag_cities_)\n",
    "        \n",
    "    def __getitem__(self, idx: int):\n",
    "        input_dict = {\n",
    "            'lag_cities_': torch.tensor(self.lag_cities_[idx], dtype=torch.long),\n",
    "            'mn': torch.tensor([self.mn[idx]], dtype=torch.long),\n",
    "            'dy1': torch.tensor([self.dy1[idx]], dtype=torch.long),\n",
    "            'dy2': torch.tensor([self.dy2[idx]], dtype=torch.long),\n",
    "            'length': torch.tensor([self.length[idx]], dtype=torch.float),\n",
    "            'trip_length': torch.tensor([self.trip_length[idx]], dtype=torch.float),\n",
    "            'N': torch.tensor([self.N[idx]], dtype=torch.float),\n",
    "            'log_icount': torch.tensor([self.log_icount[idx]], dtype=torch.float),\n",
    "            'log_dcount': torch.tensor([self.log_dcount[idx]], dtype=torch.float),\n",
    "            'lag_countries_': torch.tensor(self.lag_countries_[idx], dtype=torch.long),\n",
    "            'first_city': torch.tensor([self.first_city[idx]], dtype=torch.long),\n",
    "            'first_country': torch.tensor([self.first_country[idx]], dtype=torch.long),\n",
    "            'booker_country_': torch.tensor([self.booker_country_[idx]], dtype=torch.long),\n",
    "            'device_class_': torch.tensor([self.device_class_[idx]], dtype=torch.long),\n",
    "            'lapse': torch.tensor([self.lapse[idx]], dtype=torch.float),\n",
    "            'season': torch.tensor([self.season[idx]], dtype=torch.long),\n",
    "            'weekend': torch.tensor([self.weekend[idx]], dtype=torch.long),\n",
    "        }\n",
    "        if self.target is not None:\n",
    "            input_dict['target'] = torch.tensor([self.target[idx]], dtype=torch.long)\n",
    "        return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BookingDataset(raw.to_pandas(), 'city_id_')\n",
    "\n",
    "dataset.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(loader, model, optimizer, scheduler, scaler, device):\n",
    "\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    train_loss = []\n",
    "    bar = tqdm(range(len(loader)))\n",
    "    load_iter = iter(loader)\n",
    "    batch = load_iter.next()\n",
    "    batch = {k:batch[k].to(device, non_blocking=True) for k in batch.keys() }\n",
    "    \n",
    "    for i in bar:\n",
    "        \n",
    "        old_batch = batch\n",
    "        if i + 1 < len(loader):\n",
    "            batch = load_iter.next()\n",
    "            batch = {k:batch[k].to(device, non_blocking=True) for k in batch.keys() }\n",
    "                    \n",
    "\n",
    "        out_dict = model(old_batch)\n",
    "        logits = out_dict['logits']\n",
    "        loss = out_dict['loss']              \n",
    "        loss_np = loss.detach().cpu().numpy()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        for p in model.parameters(): \n",
    "            p.grad = None\n",
    "\n",
    "        train_loss.append(loss_np)\n",
    "        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n",
    "        bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def val_epoch(loader, model, device):\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    LOGITS = []\n",
    "    TARGETS = []\n",
    "       \n",
    "    with torch.no_grad():\n",
    "        bar = tqdm(range(len(loader)))\n",
    "        load_iter = iter(loader)\n",
    "        batch = load_iter.next()\n",
    "        batch = {k:batch[k].to(device, non_blocking=True) for k in batch.keys() }\n",
    "\n",
    "\n",
    "        for i in bar:\n",
    "\n",
    "            old_batch = batch\n",
    "            if i + 1 < len(loader):\n",
    "                batch = load_iter.next()\n",
    "                batch = {k:batch[k].to(device, non_blocking=True) for k in batch.keys() }\n",
    "\n",
    "            out_dict = model(old_batch)\n",
    "            logits = out_dict['logits']\n",
    "            loss = out_dict['loss']              \n",
    "            loss_np = loss.detach().cpu().numpy()\n",
    "            target = old_batch['target']\n",
    "            LOGITS.append(logits.detach())\n",
    "            TARGETS.append(target.detach())\n",
    "            val_loss.append(loss_np) \n",
    "\n",
    "            smooth_loss = sum(val_loss[-100:]) / min(len(val_loss), 100)\n",
    "            bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n",
    "\n",
    "        val_loss = np.mean(val_loss)\n",
    "\n",
    "    LOGITS = torch.cat(LOGITS).cpu().numpy()\n",
    "    TARGETS = torch.cat(TARGETS).cpu().numpy()\n",
    "    \n",
    "    return val_loss, LOGITS, TARGETS\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, scaler, best_score, fold, seed, fname):\n",
    "    checkpoint = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'scaler': scaler.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'best_score': best_score,\n",
    "        }\n",
    "    torch.save(checkpoint, './checkpoints/%s/%s_%d_%d.pt' % (fname, fname, fold, seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(fold, seed, device, fname):\n",
    "    model = Net(NUM_CITIES+1, NUM_HOTELS+1, EMBEDDING_DIM, HIDDEN_DIM, dropout_rate=DROPOUT_RATE,\n",
    "                loss=False).to(device)\n",
    " \n",
    "    checkpoint = torch.load('./checkpoints/%s/%s_%d_%d.pt' % (fname, fname, fold, seed))\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = torch.nn.CrossEntropyLoss(ignore_index=LOW_CITY)\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_cities, num_countries, embedding_dim, hidden_dim, dropout_rate, loss=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.loss = loss\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.cities_embeddings = nn.Embedding(num_cities, embedding_dim)     \n",
    "        self.cities_embeddings.weight.data.normal_(0., 0.01)\n",
    "        print('city embedding data shape', self.cities_embeddings.weight.shape)\n",
    "\n",
    "        self.countries_embeddings = nn.Embedding(num_countries, embedding_dim)     \n",
    "        self.countries_embeddings.weight.data.normal_(0., 0.01)\n",
    "        print('country embedding data shape', self.countries_embeddings.weight.shape)\n",
    "\n",
    "        self.mn_embeddings = nn.Embedding(12, embedding_dim)     \n",
    "        self.mn_embeddings.weight.data.normal_(0., 0.01)\n",
    "\n",
    "        self.dy1_embeddings = nn.Embedding(7, embedding_dim)     \n",
    "        self.dy1_embeddings.weight.data.normal_(0., 0.01)\n",
    "\n",
    "        self.dy2_embeddings = nn.Embedding(7, embedding_dim)     \n",
    "        self.dy2_embeddings.weight.data.normal_(0., 0.01)\n",
    "        \n",
    "        #self.season_embeddings = nn.Embedding(7, embedding_dim)     \n",
    "        #self.season_embeddings.weight.data.normal_(0., 0.01)\n",
    "        \n",
    "        self.weekend_embeddings = nn.Embedding(2, embedding_dim)     \n",
    "        self.weekend_embeddings.weight.data.normal_(0., 0.01)\n",
    "        \n",
    "        self.linear_length = nn.Linear(1, embedding_dim, bias=False)\n",
    "        self.norm_length = nn.BatchNorm1d(embedding_dim)\n",
    "        self.activate_length = nn.ReLU()\n",
    "        \n",
    "        self.linear_trip_length = nn.Linear(1, embedding_dim, bias=False)\n",
    "        self.norm_trip_length = nn.BatchNorm1d(embedding_dim)\n",
    "        self.activate_trip_length = nn.ReLU()\n",
    "\n",
    "        self.linear_N = nn.Linear(1, embedding_dim, bias=False)\n",
    "        self.norm_N = nn.BatchNorm1d(embedding_dim)\n",
    "        self.activate_N = nn.ReLU()\n",
    "\n",
    "        self.linear_log_icount = nn.Linear(1, embedding_dim, bias=False)\n",
    "        self.norm_log_icount = nn.BatchNorm1d(embedding_dim)\n",
    "        self.activate_log_icount = nn.ReLU()\n",
    "\n",
    "        self.linear_log_dcount = nn.Linear(1, embedding_dim, bias=False)\n",
    "        self.norm_log_dcount = nn.BatchNorm1d(embedding_dim)\n",
    "        self.activate_log_dcount = nn.ReLU()\n",
    "\n",
    "        self.devices_embeddings = nn.Embedding(NUM_DEVICE, embedding_dim)     \n",
    "        self.devices_embeddings.weight.data.normal_(0., 0.01)\n",
    "        print('device_embeddings data shape', self.devices_embeddings.weight.shape)\n",
    "\n",
    "        self.linear_lapse = nn.Linear(1, embedding_dim, bias=False)\n",
    "        self.norm_lapse = nn.BatchNorm1d(embedding_dim)\n",
    "        self.activate_lapse = nn.ReLU()\n",
    "        \n",
    "        self.linear1 = nn.Linear((len(lag_cities) + len(lag_countries) + 1)*embedding_dim, hidden_dim)\n",
    "        self.norm1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.activate1 = nn.PReLU()\n",
    "        self.dropout1 = nn.Dropout(self.dropout_rate)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.norm2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.activate2 = nn.PReLU()\n",
    "        self.dropout2 = nn.Dropout(self.dropout_rate)\n",
    "        self.linear3 = nn.Linear(hidden_dim, embedding_dim)\n",
    "        self.norm3 = nn.BatchNorm1d(embedding_dim)\n",
    "        self.activate3 = nn.PReLU()\n",
    "        self.dropout3 = nn.Dropout(self.dropout_rate)\n",
    "        self.output_layer_bias = nn.Parameter(torch.Tensor(num_cities, ))\n",
    "        self.output_layer_bias.data.normal_(0., 0.01)\n",
    "        \n",
    "    def get_embed(self, x, embed):\n",
    "        bs = x.shape[0]\n",
    "        x = embed(x)      \n",
    "        # lag_embed.shape: bs, x.shape[1], embedding_dim\n",
    "        x = x.view(bs, -1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input_dict):\n",
    "        lag_embed = self.get_embed(input_dict['lag_cities_'], self.cities_embeddings)      \n",
    "        lag_countries_embed = self.get_embed(input_dict['lag_countries_'], self.countries_embeddings)      \n",
    "        mn_embed = self.get_embed(input_dict['mn'], self.mn_embeddings)      \n",
    "        dy1_embed = self.get_embed(input_dict['dy1'], self.dy1_embeddings)      \n",
    "        dy2_embed = self.get_embed(input_dict['dy2'], self.dy2_embeddings)  \n",
    "        #season_embed = self.get_embed(input_dict['season'], self.season_embeddings)  \n",
    "        weekend_embed = self.get_embed(input_dict['weekend'], self.weekend_embeddings)  \n",
    "        length = input_dict['length']\n",
    "        length_embed = self.activate_length(self.norm_length(self.linear_length(length)))\n",
    "        trip_length = input_dict['trip_length']\n",
    "        trip_length_embed = self.activate_trip_length(self.norm_trip_length(self.linear_trip_length(trip_length)))\n",
    "        N = input_dict['N']\n",
    "        N_embed = self.activate_N(self.norm_N(self.linear_N(N)))\n",
    "        lapse = input_dict['lapse']\n",
    "        lapse_embed = self.activate_lapse(self.norm_lapse(self.linear_lapse(lapse)))\n",
    "        log_icount = input_dict['log_icount']\n",
    "        log_icount_embed = self.activate_log_icount(self.norm_log_icount(self.linear_log_icount(log_icount)))\n",
    "        log_dcount = input_dict['length']\n",
    "        log_dcount_embed = self.activate_log_dcount(self.norm_log_dcount(self.linear_log_dcount(log_dcount)))\n",
    "        first_city_embed = self.get_embed(input_dict['first_city'], self.cities_embeddings)  \n",
    "        first_country_embed = self.get_embed(input_dict['first_country'], self.countries_embeddings)  \n",
    "        booker_country_embed = self.get_embed(input_dict['booker_country_'], self.countries_embeddings)  \n",
    "        device_embed = self.get_embed(input_dict['device_class_'], self.devices_embeddings)  \n",
    "        x = (mn_embed + dy1_embed + dy2_embed + length_embed + log_icount_embed + log_dcount_embed \\\n",
    "             + first_city_embed + first_country_embed + booker_country_embed + device_embed \\\n",
    "             + trip_length_embed + N_embed + lapse_embed + weekend_embed)\n",
    "        x = torch.cat([lag_embed, lag_countries_embed, x], -1)\n",
    "        x = self.activate1(self.norm1(self.linear1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = x + self.activate2(self.norm2(self.linear2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.activate3(self.norm3(self.linear3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        logits = F.linear(x, self.cities_embeddings.weight, bias=self.output_layer_bias)\n",
    "        output_dict = {\n",
    "            'logits':logits\n",
    "                      }\n",
    "        if self.loss:\n",
    "            target = input_dict['target'].squeeze(1)\n",
    "            #print(logits.shape, target.shape)\n",
    "            loss = loss_fct(logits, target)\n",
    "            output_dict['loss'] = loss\n",
    "        return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 1024\n",
    "WORKERS = 8\n",
    "LR = 1e-3\n",
    "EPOCHS = 12\n",
    "GRADIENT_ACCUMULATION = 1\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM =  1024\n",
    "DROPOUT_RATE = 0.2\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top4(preds):\n",
    "    TOP4 = np.empty((preds.shape[0], 4))\n",
    "    for i in range(4):\n",
    "        x = np.argmax(preds, axis=1)\n",
    "        TOP4[:,i] = x\n",
    "        x = np.expand_dims(x, axis=1)\n",
    "        np.put_along_axis(preds, x, -1e10, axis=1)\n",
    "    return TOP4\n",
    "\n",
    "def top4(preds, target):\n",
    "    TOP4 = get_top4(preds)\n",
    "    acc = np.max(TOP4 == target, axis=1)\n",
    "    acc = np.mean(acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_WITH_TEST = True\n",
    "\n",
    "seed = 0\n",
    "seed_torch(seed)\n",
    "\n",
    "preds_all = []\n",
    "best_scores = []\n",
    "best_epochs = []\n",
    "for fold in range(5):\n",
    "\n",
    "    seed_torch(seed)\n",
    "    preds_fold = []\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold))\n",
    "    if TRAIN_WITH_TEST:\n",
    "        train = raw.loc[ (raw.fold!=fold)&(raw.dcount>0)&(raw.istest==0)|( (raw.istest==1)&(raw.icount>0) ) ].copy()\n",
    "    else:\n",
    "        train = raw.loc[ (raw.fold!=fold)&(raw.dcount>0)&(raw.istest==0) ].copy()\n",
    "    valid = raw.loc[ (raw.fold==fold)&(raw.istest==0)&(raw.icount==0) &(raw.reverse == 0)].copy()\n",
    "    print(train.shape, valid.shape)\n",
    "\n",
    "    train_dataset = BookingDataset(train.to_pandas(), target='city_id_')\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        num_workers=WORKERS,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    valid_dataset = BookingDataset(valid.to_pandas(), target='city_id_')\n",
    "\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        num_workers=WORKERS,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "\n",
    "    model = Net(NUM_CITIES+1, NUM_HOTELS+1, EMBEDDING_DIM, HIDDEN_DIM, dropout_rate=DROPOUT_RATE).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, \n",
    "                                              pct_start=0.1, \n",
    "                                              div_factor=1e3, \n",
    "                                              max_lr=3e-3, \n",
    "                                              epochs=EPOCHS, \n",
    "                                              steps_per_epoch=int(np.ceil(len(train_data_loader)/GRADIENT_ACCUMULATION)))\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_score = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(time.ctime(), 'Epoch:', epoch, flush=True)\n",
    "        train_loss = train_epoch(train_data_loader, model, optimizer, scheduler, scaler, device)\n",
    "        val_loss, PREDS, TARGETS = val_epoch(valid_data_loader, model, device) \n",
    "        PREDS[:, LOW_CITY] = -1e10# remove low frequency cities\n",
    "        score = top4(PREDS, TARGETS)\n",
    "\n",
    "        content = 'Fold %d Seed %d Ep %d lr %.7f train loss %4f val loss %4f score %4f'\n",
    "        print(content % (fold, seed, epoch, \n",
    "                         optimizer.param_groups[0][\"lr\"],\n",
    "                         np.mean(train_loss),\n",
    "                         np.mean(val_loss),\n",
    "                         score,\n",
    "                        ), \n",
    "              flush=True)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_epoch = epoch\n",
    "            preds_fold = PREDS\n",
    "            save_checkpoint(model, optimizer, scaler, scheduler, best_score, fold, seed, fname)\n",
    "    del model, scaler, scheduler, optimizer, valid_data_loader, valid_dataset, train_data_loader, train_dataset\n",
    "    gc.collect()\n",
    "\n",
    "    preds_all.append(preds_fold)\n",
    "    print('fold %d, best score: %0.6f best epoch: %3d' % (fold, best_score, best_epoch))\n",
    "    best_scores.append(best_score)\n",
    "    best_epochs.append(best_epoch)\n",
    "    #with open('../checkpoints/%s/%s_%d_preds.pkl' % (fname, fname, seed), 'wb') as file:\n",
    "    #    pkl.dump(preds_all, file)\n",
    "        \n",
    "    #break\n",
    "print()\n",
    "for fold, (best_score, best_epoch) in enumerate(zip(best_scores, best_epochs)):\n",
    "    print('fold %d, best score: %0.6f best epoch: %3d' % (fold, best_score, best_epoch))\n",
    "print('seed %d best score: %0.6f best epoch: %0.1f' % (seed, np.mean(best_scores), np.mean(best_epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(loader, models, device):\n",
    "\n",
    "    #model.eval()\n",
    "    PREDS = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if 1:\n",
    "            bar = tqdm(range(len(loader)))\n",
    "            load_iter = iter(loader)\n",
    "            batch = load_iter.next()\n",
    "            batch = {k:batch[k].to(device, non_blocking=True) for k in batch.keys() }\n",
    "\n",
    "            for i in bar:\n",
    "                old_batch = batch\n",
    "                if i + 1 < len(loader):\n",
    "                    batch = load_iter.next()\n",
    "                    batch = {k:batch[k].to(device, non_blocking=True) for k in batch.keys() }\n",
    "                    \n",
    "                preds = 0\n",
    "                for model in models:\n",
    "                    out_dict = model(old_batch)\n",
    "                    preds = preds + out_dict['logits'] / NFOLDS\n",
    "                PREDS.append(preds.detach())\n",
    "                \n",
    "    \n",
    "    PREDS = torch.cat(PREDS).cpu().numpy()\n",
    "    \n",
    "    return PREDS\n",
    "\n",
    "NFOLDS = 5\n",
    "seed = 0\n",
    "models = [load_checkpoint(fold, seed, device, fname) for fold in range(NFOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test N-1 (leaky as we train using all test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(valid):\n",
    "    acc = cupy.max(valid[COLS[1:]].values == valid[['city_id']].values, axis=1)\n",
    "    acc = cupy.mean(acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = raw.loc[ (raw.istest==1)&(raw.icount==1) ].copy()\n",
    "print( test.shape )\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = BookingDataset(test.to_pandas(), target=None)\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    num_workers=WORKERS,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "PREDS = test_epoch(test_data_loader, models, device) \n",
    "PREDS[:, LOW_CITY] = -1e10# remove low frequency cities\n",
    "TOP4 = get_top4(PREDS).astype('int')\n",
    "TOP4.shape\n",
    "\n",
    "COLS = ['utrip_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mapping = MAPS[0].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITY_MAP = MAPS[0].astype('int')\n",
    "for k in range(4):\n",
    "    test['city_id_%i'%(k+1)] = TOP4[:,k]\n",
    "    tmp = test[['city_id_%i'%(k+1)]].astype('int32').copy()\n",
    "    tmp['sorting'] = cupy.asarray(range(tmp.shape[0]))\n",
    "    tmp = tmp.merge(city_mapping, how='left', left_on='city_id_%i'%(k+1), right_on='index')\n",
    "    tmp = tmp.sort_values('sorting')\n",
    "    test['city_id_%i'%(k+1)] = tmp['city_id'].astype('int32').values.copy()\n",
    "    COLS.append('city_id_%i'%(k+1))\n",
    "    \n",
    "test[COLS].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = raw.loc[ (raw.istest==1)&(raw.icount==0)&(raw.reverse==0) ].copy()\n",
    "print( test.shape )\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = BookingDataset(test.to_pandas(), target=None)\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    num_workers=WORKERS,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "PREDS = test_epoch(test_data_loader, models, device) \n",
    "PREDS[:, LOW_CITY] = -1e10# remove low frequency cities\n",
    "TOP4 = get_top4(PREDS).astype('int')\n",
    "TOP4.shape\n",
    "\n",
    "COLS = ['utrip_id']\n",
    "CITY_MAP = MAPS[0].astype('int')\n",
    "for k in range(4):\n",
    "    test['city_id_%i'%(k+1)] = TOP4[:,k]\n",
    "    tmp = test[['city_id_%i'%(k+1)]].astype('int32').copy()\n",
    "    tmp['sorting'] = cupy.asarray(range(tmp.shape[0]))\n",
    "    tmp = tmp.merge(city_mapping, how='left', left_on='city_id_%i'%(k+1), right_on='index')\n",
    "    tmp = tmp.sort_values('sorting')\n",
    "    test['city_id_%i'%(k+1)] = tmp['city_id'].astype('int32').values.copy()\n",
    "    COLS.append('city_id_%i'%(k+1))\n",
    "    \n",
    "test[COLS].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[COLS].to_csv('%s_sub.csv' % fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOF Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(fold, seed, device, fname, loss=False):\n",
    "    model = Net(NUM_CITIES+1, NUM_HOTELS+1, EMBEDDING_DIM, HIDDEN_DIM, dropout_rate=DROPOUT_RATE,\n",
    "                loss=loss).to(device)\n",
    " \n",
    "    checkpoint = torch.load('./checkpoints/%s/%s_%d_%d.pt' % (fname, fname, fold, seed))\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_topN(preds, N):\n",
    "    TOPN = np.empty((preds.shape[0], N))\n",
    "    PREDN = np.empty((preds.shape[0], N))\n",
    "    preds = preds.copy()\n",
    "    for i in tqdm(range(N)):\n",
    "        x = np.argmax(preds, axis=1)\n",
    "        TOPN[:,i] = x\n",
    "        x = np.expand_dims(x, axis=1)\n",
    "        PREDN[:,i] = np.take_along_axis(preds, x, axis=1).ravel()\n",
    "        np.put_along_axis(preds, x, -1e10, axis=1)\n",
    "    return TOPN, PREDN\n",
    "\n",
    "def get_top4(preds):\n",
    "    preds = preds.copy()\n",
    "    TOP4 = np.empty((preds.shape[0], 4))\n",
    "    for i in range(4):\n",
    "        x = np.argmax(preds, axis=1)\n",
    "        TOP4[:,i] = x\n",
    "        x = np.expand_dims(x, axis=1)\n",
    "        np.put_along_axis(preds, x, -1e10, axis=1)\n",
    "    return TOP4\n",
    "\n",
    "def top4(preds, target):\n",
    "    TOP4 = get_top4(preds)\n",
    "    acc = np.max(TOP4 == target, axis=1)\n",
    "    acc = np.mean(acc)\n",
    "    return acc\n",
    "\n",
    "TRAIN_WITH_TEST = True\n",
    "\n",
    "seed = 0\n",
    "seed_torch(seed)\n",
    "\n",
    "preds_all = []\n",
    "test_preds_all = []\n",
    "train_all = []\n",
    "best_scores = []\n",
    "for fold in range(1):\n",
    "\n",
    "    seed_torch(seed)\n",
    "    preds_fold = []\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold))\n",
    "    valid = raw.loc[ (raw.fold==fold)&(raw.istest==0)&(raw.icount==0) &(raw.reverse == 0)].copy()\n",
    "    print(valid.shape)\n",
    "\n",
    "    valid_dataset = BookingDataset(valid.to_pandas(), target='city_id_')\n",
    "\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        num_workers=WORKERS,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    test_dataset = BookingDataset(test.to_pandas(), target=None)\n",
    "\n",
    "    test_data_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        num_workers=WORKERS,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    \n",
    "    model = load_checkpoint(fold, seed, device, fname, loss=True)\n",
    "    val_loss, PREDS, TARGETS = val_epoch(valid_data_loader, model, device) \n",
    "    PREDS[:, LOW_CITY] = -1e10# remove low frequency cities\n",
    "    score = top4(PREDS, TARGETS)\n",
    "    print('fold %d, best score: %0.6f' % (fold, score))\n",
    "    score = top4(PREDS, valid[['city_id_']].to_pandas().values)\n",
    "    print('fold %d, best score: %0.6f' % (fold, score))\n",
    "    best_scores.append(score)\n",
    "    preds_all.append(PREDS)\n",
    "    train_all.append(valid.to_pandas())\n",
    "\n",
    "    model = load_checkpoint(fold, seed, device, fname, loss=False)\n",
    "    TEST_PREDS = test_epoch(test_data_loader, [model], device) \n",
    "    TEST_PREDS[:, LOW_CITY] = -1e10# remove low frequency cities\n",
    "    test_preds_all.append(TEST_PREDS)\n",
    "print('seed %d best score: %0.6f' % (seed, np.mean(best_scores)))\n",
    "\n",
    "preds_all = np.concatenate(preds_all)\n",
    "print('vaid pred shape', preds_all.shape)\n",
    "\n",
    "test_preds_all = np.mean(test_preds_all, axis=0)\n",
    "print('test pred shape', test_preds_all.shape)\n",
    "\n",
    "top_preds, top_logits = get_topN(preds_all, 4)\n",
    "targets = np.concatenate([valid[['city_id_']].values for valid in train_all])\n",
    "valid_trips = np.concatenate([valid[['utrip_id']].values for valid in train_all])\n",
    "\n",
    "print('CV score', np.mean(np.max(top_preds == targets, axis=1)))\n",
    "\n",
    "valid_trips\n",
    "\n",
    "top_preds, top_logits = get_topN(preds_all, 50)\n",
    "top_test_preds, top_test_logits = get_topN(test_preds_all, 50)\n",
    "\n",
    "\n",
    "top_preds.shape, top_logits.shape, top_test_preds.shape, top_test_logits.shape\n",
    "\n",
    "valid_cities = np.concatenate([valid[['city_id']].values for valid in train_all])\n",
    "oof = {\n",
    "    'valid_trips':valid_trips,\n",
    "    'top_preds':top_preds,\n",
    "    'top_logits':top_logits,\n",
    "    'top_test_preds':top_test_preds,\n",
    "    'top_test_logits':top_test_logits,\n",
    "    'city_map':CITY_MAP,\n",
    "    'valid_cities':valid_cities,\n",
    "}\n",
    "\n",
    "with open((checkpoint_path / (fname + '_oof.pkl')), 'wb') as file:\n",
    "    pkl.dump(oof, file)\n",
    "    \n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = {\n",
    "    'valid_trips':valid_trips,\n",
    "    'top_preds':top_preds,\n",
    "    'top_logits':top_logits,\n",
    "    'top_test_preds':top_test_preds,\n",
    "    'top_test_logits':top_test_logits,\n",
    "    'city_map':CITY_MAP,\n",
    "    'valid_cities':valid_cities,\n",
    "    'preds_all':preds_all,\n",
    "    'test_preds_all':test_preds_all,\n",
    "}\n",
    "\n",
    "with open((checkpoint_path / (fname + '_oof.pkl')), 'wb') as file:\n",
    "    pkl.dump(oof, file, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "    \n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
